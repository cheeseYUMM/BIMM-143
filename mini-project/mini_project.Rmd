---
title: 'Class 8 : Mini Project'
author: "Soobin (PID:A15201229)"
date: "2/10/2022"
output: pdf_document
---

# Unsupervised Learning Analysis of Human Breast Cancer Cells

Here we read data from the University of Wisconsin Medical Center on breast cancer patients
```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)
```

Examine wisc.df
```{r}
head(wisc.df)
nrow(wisc.df)
ncol(wisc.df)
```
> Q1. How many observations are in this dataset?
There are 569 rows and 31 columns

Create a new data.frame that omits the first column
```{r}
wisc.data <- wisc.df[, -1]
head(wisc.data)
```

Setup a separate new vector called diagnosis that contains the data from the diagnosis column of the original dataset.
```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
table(diagnosis)
```

> Q2. How many of the observations have a malignant diagnosis?
212 observations have malignant diagnosis. 

First, we will examine the column names
```{r}
wanted_colnames <- colnames(wisc.df)
```

We will use grep() function to find features that have suffix "_mean"
```{r}
length(grep(wanted_colnames, pattern = "_mean"))
```

> Q3. How many variables/features in the data are suffixed with "_mean"?
There are 10 variables in the data that are suffixed with "_mean".

## Principal Component Analysis (PCA)

We will check the mean and standard deviation of the features of wisc.data
```{r}
colMeans(wisc.data)
apply(wisc.data, 2, sd)
```

Here we need to scale the data before PCA as the various variables (i.e. columns) have very different scales.
```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```
> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
PC1 captures 44.27% of the variance.

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
3 PCs are required to describe at least 70% of the original variance in the data.

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
7 PCs are required to describe at least 90% of the original variance in the data.  

## Interpreting PCA results
```{r}
biplot(wisc.pr)
```

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?
The rownames and observation overlap each other, which is difficult to understand this plot. The plot contains non-trivial numbers of observations and variables that there are other visualalization methods that better represent the data.

Now I will make my main result: The "PCA plot" (a.k.a. "score plot", PC1 and PC2 plot) --> Healthy individuals have similar cell characteristics
```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?
The plot for PC1 and PC2 has a cleaner cut separating the two subgroups because PC2 explains more variance in the original data than principal component 3. But the plot for PC1 and PC3 also shows that PC1 captures a separation of malignant (red) and benign (black) data. 

```{r}
plot(wisc.pr$x[,c(1,3)], col=diagnosis)
```

We will use a ggplot() to create a graph that is more aesthetic.
```{r}
# load ggplot2
library(ggplot2)

# ggplot only takes data.frame
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Make a scatterplot
ggplot(df, aes(x = wisc.pr$x[,1], y = wisc.pr$x[,2], col = diagnosis)) + geom_point() + labs(x = "PC1", y = "PC2") 
```

## Variance explained

Calculate the variance of each principal component by squaring the sdev component of wisc.pr
```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

Calculate the variance explained by each principal component by dividing by the total variance explained of all principal components
```{r}
pve <- pr.var / sum(pr.var)
```

Plot the variance explained for each principal component
```{r}
# type = "p" (only point)
# type = "l" (only line)
# type = "b" (both point and line, not passing through)
# type = "o" (both point and line, passing through))
plot(pve, ylab = "Proportion of Variance Explained", xlab = "Principal Component", ylim = c(0, 1), type = "o")
```

Plot a bargraph
```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
## ggplot based graph
# install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?
The component of the loading vector for the feature concave.points_mean is -0.26085376.

> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?
5 PCs are required to explain 80% of the variance of the data.


## Hierarchical Clustering

```{r}
# Scale the wisc.data using the scale() function
data.scaled <- scale(wisc.data)
```

```{r}
# Calculate the Euclidean distance between all pairs of observation
data.dist <- dist(data.scaled)
```

```{r}
wisc.hclust <- hclust(data.dist, "complete")
```

### Results of hierarchical clustering

> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
When h = 19, the clustering model will have 4 clusters.

```{r}
plot(wisc.hclust)
abline(h=19, col="red")
```

### Selecting number of clusters

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters, diagnosis)
```

> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?
No, 4 clusters shows the best cluster vs. diagnoses match.

### Using different methods

```{r}
wisc.hclust <- hclust(data.dist, "complete")
wisc.hclust1 <- hclust(data.dist, "single")
wisc.hclust2 <- hclust(data.dist, "average")
wisc.hclust3 <- hclust(data.dist, "ward.D2")
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters, diagnosis)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust1, k=4)
table(wisc.hclust.clusters, diagnosis)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust2, k=4)
table(wisc.hclust.clusters, diagnosis)
```


```{r}
wisc.hclust.clusters <- cutree(wisc.hclust3, k=4)
table(wisc.hclust.clusters, diagnosis)
```

>Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.
"complete" and "ward.D2" methods both distributes the  clusters pretty well. I personally prefer "ward.D2" method because I like how it gives minimum increase in total within-cluster variance. Also it is Dr.Grant's favorite! XD

## OPTIONAL: K-means clustering

### K-means clustering and comparing results

```{r}
wisc.km <- kmeans( scale(wisc.data), centers = 2, nstart=20 )
wisc.km
```

```{r}
table(wisc.km$cluster, diagnosis)
```

>Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?
I think k-means did a decent job in separating two diagnoses. Looking at the table, it looks like the hierarchical clustering model assigns most of the observations to cluster 1 and cluster 4, while the k-means algorithm distributes the observations relatively evenly among all clusters. 

```{r}
table(wisc.hclust.clusters, wisc.km$cluster)
```
## Combining methods

First let's try clustering the raw data
```{r}
hc <- hclust( dist(wisc.data) )
plot(hc)
```

We can combine methods to be useful. We can take our PCA results and apply clustering to them.

Here we will take the first 7 PCs for clustering
```{r}
pcdist <- wisc.pr$x[, 1:7]
wisc.pr.hclust <- hclust( dist(pcdist), method="ward.D2" )
plot(wisc.pr.hclust)
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(diagnosis)
```

```{r}
table(diagnosis, grps)
```
>Q15. How well does the newly created model with two clusters separate out the two diagnoses?
The newly created model with two clusters separate out the two diagnosis pretty well. 

```{r}
table(grps, diagnosis)
```

> Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses?
I think both clustering models before PCA did a decent job in separating the diagnoses. 

```{r}
table(wisc.km$cluster, diagnosis)
table(wisc.hclust.clusters, diagnosis)
```


```{r}
plot(wisc.pr$x[,1:2], col=grps)
```
```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
# plot using our re-ordered factor
plot( wisc.pr$x[ , 1:2], col=g )
```

## Sensitivity/Specificity

> Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?
hierarchical clustering have best specificity, while k-means have best sensitivity.

## Prediction
```{r}
url <- "new_samples.csv"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```
> Q18. Which of these new patients should we prioritize for follow up based on your results?
#1 represents individuals with benign cells and #2 represents individuals with malignant cells. Therefore, we would have to priooritize patients in #2.
